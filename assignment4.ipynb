{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import itertools \n",
    "import random\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = []\n",
    "random.seed(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen lake problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True)\n",
    "observation = env.reset()\n",
    "env.env.s = 62 # go to state s\n",
    "s_next, r, done, info = env.step(3)\n",
    "env.render()\n",
    "env.close()\n",
    "print(\"Next state: {} Reward: {} Done: {} Info: {}\".format(s_next, r, done, info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/realdiganta/solving_openai/blob/master/FrozenLake8x8/frozenLake8x8.py\n",
    "def value_iteration(env, max_iterations=100000, lmbda=0.9, verbose=1):\n",
    "    stateValue = [0 for i in range(env.nS)]\n",
    "    newStateValue = stateValue.copy()\n",
    "    iterations = 0\n",
    "    start_time = time.time()\n",
    "    state_values = []\n",
    "    for i in range(max_iterations):\n",
    "        \n",
    "        if verbose>0: print(\"Iteration: {}\".format(i))\n",
    "        \n",
    "        for state in range(env.nS):\n",
    "            action_values = []      \n",
    "            \n",
    "            for action in range(env.nA):\n",
    "                state_value = 0\n",
    "                \n",
    "                for s in range(len(env.P[state][action])):\n",
    "                    prob, next_state, reward, done = env.P[state][action][s]\n",
    "                    \n",
    "                    if verbose>1: \n",
    "                        print(\"Iteration: {} State: {} Action: {} Prob: {} Next State: {} Reward: {}\".format(i, state, action, prob, next_state, reward))\n",
    "                    state_action_value = prob * (reward + lmbda*stateValue[next_state])\n",
    "                    state_value += state_action_value\n",
    "                \n",
    "                action_values.append(state_value)\n",
    "                best_action = np.argmax(np.asarray(action_values))\n",
    "                newStateValue[state] = action_values[best_action]\n",
    "        \n",
    "        # if i > 1000: \n",
    "        if verbose>0: print(\"Delta V: {}\".format(np.max(np.abs(np.array(stateValue) - np.array(newStateValue)))))\n",
    "        if np.max(np.abs(np.array(stateValue) - np.array(newStateValue))) < 0.0001:\n",
    "            break\n",
    "            \n",
    "        stateValue = newStateValue.copy()\n",
    "        iterations += 1\n",
    "        state_values.append(sum(stateValue))\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    return stateValue, iterations, duration, state_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/realdiganta/solving_openai/blob/master/FrozenLake8x8/frozenLake8x8.py\n",
    "def get_policy(env,stateValue, lmbda=0.9):\n",
    "    policy = [0 for i in range(env.nS)]\n",
    "    for state in range(env.nS):\n",
    "        action_values = []\n",
    "        for action in range(env.nA):\n",
    "            action_value = 0\n",
    "            # print(env.P[state][action])\n",
    "            for i in range(len(env.P[state][action])):\n",
    "                prob, next_state, r, _ = env.P[state][action][i]\n",
    "                # print(\"I: {} R: {}\".format(i, r))\n",
    "                action_value += prob * (r + lmbda * stateValue[next_state])\n",
    "            action_values.append(action_value)\n",
    "        best_action = np.argmax(np.asarray(action_values))\n",
    "        policy[state] = best_action\n",
    "        # print(\"State: {} Action Values: {} Best Action: {}\".format(state, action_values, best_action))\n",
    "    return policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/realdiganta/solving_openai/blob/master/FrozenLake8x8/frozenLake8x8.py\n",
    "def get_score(env, policy, episodes=1000):\n",
    "  misses = 0\n",
    "  steps_list = []\n",
    "  for episode in range(episodes):\n",
    "    observation = env.reset()\n",
    "    steps=0\n",
    "    while True:\n",
    "      \n",
    "      action = policy[observation]\n",
    "      observation, reward, done, _ = env.step(action)\n",
    "      steps+=1\n",
    "      if done and reward == 1:\n",
    "        # print('You have got the Frisbee after {} steps'.format(steps))\n",
    "        steps_list.append(steps)\n",
    "        break\n",
    "      elif done and reward == 0:\n",
    "        # print(\"You fell in a hole!\")\n",
    "        misses += 1\n",
    "        break\n",
    "  print('----------------------------------------------')\n",
    "  print('You took an average of {:.0f} steps to get the frisbee'.format(np.mean(steps_list)))\n",
    "  print('And you fell in the hole {:.2f} % of the times'.format((misses/episodes) * 100))\n",
    "  print('----------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "stateValues, iterations, duration, all_state_values = value_iteration(env, max_iterations=100000, verbose=0)\n",
    "policy_value_iteration = get_policy(env, stateValues)\n",
    "get_score(env, policy_value_iteration,episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations)), 'V': all_state_values})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Frozen Lake Value Iteration Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data=[]\n",
    "evaluation_data.append(('Frozen Lake', 'Value Iteration', sum(stateValues), iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state_values(V, env, title):\n",
    "    nb_states = env.observation_space.n\n",
    "    V = np.round(V, decimals=4)\n",
    "    ax = sns.heatmap(V.reshape(int(np.sqrt(nb_states)),int(np.sqrt(nb_states))), \n",
    "                 linewidth=0.5,\n",
    "                 annot=V.reshape(int(np.sqrt(nb_states)),int(np.sqrt(nb_states))), \n",
    "                 fmt = '',\n",
    "                 cbar=False).set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state_values(stateValues, env, 'Value Iteration Frozen Lake Values')\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state_values(policy_value_iteration, env, 'Value Iteration Frozen Lake Policy (0123 >>> LDRU)')\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def compute_vpi(pi, env, gamma):\n",
    "    # use pi[state] to access the action that's prescribed by this policy\n",
    "    \n",
    "    a = np.identity(env.nS) \n",
    "    b = np.zeros(env.nS) \n",
    "    \n",
    "    # print(a)\n",
    "    # print(b)\n",
    "\n",
    "    for state in range(env.nS):\n",
    "        for probability, nextstate, reward, done in env.P[state][pi[state]]:\n",
    "            a[state][nextstate] = a[state][nextstate] - gamma * probability\n",
    "            b[state] += probability * reward\n",
    "\n",
    "    # print(a)\n",
    "    # print(b)\n",
    "    \n",
    "    V = np.linalg.solve(a, b)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.arange(64) % env.nA\n",
    "Vpi = compute_vpi(pi, env, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def compute_qpi(vpi, env, gamma):\n",
    "    Qpi = np.zeros([env.nS, env.nA])\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            for probability, nextstate, reward, done in env.P[state][action]:\n",
    "                Qpi[state][action] += probability * (reward + gamma * vpi[nextstate]) \n",
    "    return Qpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qpi = compute_qpi(Vpi, env, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def policy_iteration(env, gamma, nIt):\n",
    "    Vs = []\n",
    "    pis = []\n",
    "    pi_prev = np.zeros(env.nS, dtype='int')\n",
    "    pis.append(pi_prev)\n",
    "    print(\"Iteration | # chg actions | V[0]\")\n",
    "    print(\"----------+---------------+---------\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for it in range(nIt):        \n",
    "        \n",
    "        # you need to compute qpi which is the state-action values for current pi\n",
    "        vpi = compute_vpi(pis[-1], env, gamma=gamma)\n",
    "        qpi = compute_qpi(vpi, env, gamma=gamma)\n",
    "        pi = qpi.argmax(axis=1)\n",
    "        print(\"%4i      | %6i        | %6.5f\"%(it, (pi != pi_prev).sum(), vpi[0]))\n",
    "        Vs.append(vpi)\n",
    "        pis.append(pi)\n",
    "        pi_prev = pi\n",
    "\n",
    "        if it > 1 and np.max(np.abs(np.array(Vs[-1]) - np.array(Vs[-2]))) < 0.0001:\n",
    "            break\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    return Vs, pis, it, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_PI, pis_PI, iterations, duration = policy_iteration(env, gamma=0.9, nIt=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations + 1)), 'V': np.sum(Vs_PI, axis=1)})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Frozen Lake Policy Iteration Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data.append(('Frozen Lake', 'Policy Iteration', sum(Vs_PI[-1]), iterations + 1, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state_values(Vs_PI[-1], env, 'Policy Iteration Frozen Lake Values')\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state_values(pis_PI[-1], env, 'Policy Iteration Frozen Lake Polocy (0123 >>> LDRU)')\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(env, pis_PI[-1], episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the policy from value and policy iteration the same?\n",
    "np.array_equal(pis_PI[-1], policy_value_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "qtable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 30000        # Total episodes\n",
    "learning_rate = 0.9           # Learning rate\n",
    "max_steps = 200                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0            # Minimum exploration probability \n",
    "decay_rate = 0.01            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb\n",
    "\n",
    "LR = [0.005]\n",
    "\n",
    "for lr in LR:\n",
    "\n",
    "    qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "    sumqtable = []\n",
    "\n",
    "    total_episodes = 30000        # Total episodes\n",
    "    learning_rate = 0.9           # Learning rate\n",
    "    max_steps = 200                # Max steps per episode\n",
    "    gamma = 0.95                  # Discounting rate\n",
    "\n",
    "    # Exploration parameters\n",
    "    epsilon = 1.0                 # Exploration rate\n",
    "    max_epsilon = 1.0             # Exploration probability at start\n",
    "    min_epsilon = 0.1            # Minimum exploration probability \n",
    "    decay_rate = 0.01            # Exponential decay rate for exploration prob\n",
    "\n",
    "    decay_rate = lr\n",
    "\n",
    "    # List of rewards\n",
    "    rewards = []\n",
    "    reward_achieved = 0\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "    diffs = []\n",
    "\n",
    "    # 2 For life or until learning is stopped\n",
    "    while True:\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        old_state = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        old_qtable = qtable.copy()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # 3. Choose an action a in the current world state (s)\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = random.uniform(0, 1)\n",
    "            \n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "                # print(\"Exploiting knowledge!\")\n",
    "\n",
    "            # Else doing a random choice --> exploration\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                # print(\"Randomly exploring!\")\n",
    "\n",
    "            # print(\"Action: {}\".format(action))\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            # print(\"Episode: {} Action: {} New State: {} R: {} Epsilon: {}\".format(episode, action, new_state, reward, epsilon))\n",
    "\n",
    "            # if done and reward == 0:\n",
    "            #     reward = -0.5\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            # qtable[new_state,:] : all the actions we can take from new state\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            # Our new state is state\n",
    "            old_state = state\n",
    "            state = new_state\n",
    "            \n",
    "            # If done (if we're dead) : finish episode\n",
    "            if done == True:\n",
    "                # print(\"Done\") \n",
    "                if reward > 0:\n",
    "                    reward_achieved += 1\n",
    "                    # print(old_state, action)\n",
    "                    # print(qtable[old_state, action])\n",
    "                    # print(learning_rate * (reward + gamma * np.max(qtable[state, :]) - qtable[old_state, action]))\n",
    "                    # Reduce epsilon (because we need less and less exploration)\n",
    "                    # print(reward_achieved)\n",
    "                    # print(decay_rate)\n",
    "                    # print(np.exp(-decay_rate*reward_achieved))\n",
    "                    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*reward_achieved)\n",
    "                    # print(\"Epsilon: {}\".format(epsilon)) \n",
    "                    # print(\"Iterations: {} Action: {} New State: {} R: {} Epsilon: {}\".format(iterations, action, new_state, reward, epsilon))\n",
    "                break\n",
    "            \n",
    "        rewards.append(total_rewards)\n",
    "        iterations += 1\n",
    "\n",
    "        sumqtable.append(np.sum(qtable))\n",
    "        \n",
    "        if abs(epsilon - min_epsilon) < 0.01:\n",
    "            break\n",
    "\n",
    "        diffs.append(np.max(np.abs(np.array(old_qtable) - np.array(qtable))))\n",
    "        # print(np.std(diffs[-20:]))\n",
    "        if sum(rewards)>50 and np.std(diffs[-20:]) < 0.01:\n",
    "            break\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print (\"Number of rewards: \" +  str(sum(rewards)))\n",
    "    print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "    # print(qtable)\n",
    "\n",
    "    policy = np.argmax(qtable, axis=1)\n",
    "    get_score(env, policy,episodes=1000)\n",
    "\n",
    "    evaluation_data.append(('Frozen Lake', 'Q-learning', np.sum(qtable) / action_size, iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations)), 'V': sumqtable})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Frozen Lake Q Learning Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = [np.argmax(i) for i in qtable]\n",
    "policy = np.argmax(qtable, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state_values(policy, env, 'Frozen Lake Q-Learning Policy (0123 >>> LDRU)')\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(env, policy,episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 100\n",
    "n_actions = 4\n",
    "n_rewards = 1\n",
    "\n",
    "# select which state actions gets a reward\n",
    "n_state_actions = n_actions * n_states\n",
    "selected_state_actions = np.arange(n_state_actions)\n",
    "random.shuffle(selected_state_actions)\n",
    "selected_state_actions=selected_state_actions[:n_rewards]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_state_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "env1 = {}\n",
    "state_action = 0\n",
    "for s in range(n_states):\n",
    "    # print(s)\n",
    "    env1[s] = {}\n",
    "    \n",
    "    if s == n_states - 1:\n",
    "        next_state = 0\n",
    "    else:\n",
    "        next_state = s + 1\n",
    "\n",
    "    for a in range(n_actions):\n",
    "        # print(a)\n",
    "        if a == 0:\n",
    "            # print(\"go to next state\")\n",
    "            if state_action in selected_state_actions:\n",
    "                # print(\"add reward 1\")\n",
    "                env1[s][a] = [(1, next_state, 1, False)]\n",
    "            else:\n",
    "                # print(\"add reward 0\")\n",
    "                env1[s][a] = [(1, next_state, 0, False)]\n",
    "        else:\n",
    "            # print(\"go to random state but not next state\")\n",
    "            random_state_not_next = np.arange(n_states)\n",
    "            np.delete(s+1, random_state_not_next)\n",
    "            # print(random_state_not_next)\n",
    "            random_state_not_next = random.choice(random_state_not_next)\n",
    "            if state_action in selected_state_actions:\n",
    "                # print(\"add reward 1\")\n",
    "                env1[s][a] = [(1, random_state_not_next, 1, False)]\n",
    "            else:\n",
    "                # print(\"add reward 0\")\n",
    "                env1[s][a] = [(1, random_state_not_next, 0, False)]\n",
    "        state_action += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 10000\n",
    "n_actions = 10\n",
    "n_rewards = 2\n",
    "\n",
    "# select which state actions gets a reward\n",
    "n_state_actions = n_actions * n_states\n",
    "selected_state_actions = np.arange(n_state_actions)\n",
    "random.shuffle(selected_state_actions)\n",
    "selected_state_actions=selected_state_actions[:n_rewards]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = {}\n",
    "state_action = 0\n",
    "for s in range(n_states):\n",
    "    # print(s)\n",
    "    env2[s] = {}\n",
    "    \n",
    "    if s == n_states - 1:\n",
    "        next_state = 0\n",
    "    else:\n",
    "        next_state = s + 1\n",
    "\n",
    "    for a in range(n_actions):\n",
    "        # print(a)\n",
    "        if a == 0:\n",
    "            # print(\"go to next state\")\n",
    "            if state_action in selected_state_actions:\n",
    "                # print(\"add reward 1\")\n",
    "                env2[s][a] = [(1, next_state, 1, False)]\n",
    "            else:\n",
    "                # print(\"add reward 0\")\n",
    "                env2[s][a] = [(1, next_state, 0, False)]\n",
    "        else:\n",
    "            # print(\"go to random state but not next state\")\n",
    "            random_state_not_next = np.arange(n_states)\n",
    "            np.delete(s+1, random_state_not_next)\n",
    "            # print(random_state_not_next)\n",
    "            random_state_not_next = random.choice(random_state_not_next)\n",
    "            if state_action in selected_state_actions:\n",
    "                # print(\"add reward 1\")\n",
    "                env2[s][a] = [(1, random_state_not_next, 1, False)]\n",
    "            else:\n",
    "                # print(\"add reward 0\")\n",
    "                env2[s][a] = [(1, random_state_not_next, 0, False)]\n",
    "        state_action += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(list(env1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edges data\n",
    "edges = []\n",
    "for key_state, s in env1.items():\n",
    "    # print(key_state)\n",
    "    for key_action, a in s.items():\n",
    "        # print(a)\n",
    "        if a[0][2] != 0:\n",
    "            G.add_edge(key_state, a[0][1], reward=a[0][2])\n",
    "        else:\n",
    "            G.add_edge(key_state, a[0][1])\n",
    "        # print(key_state, a[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G)\n",
    "plt.figure(3,figsize=(15,9)) \n",
    "nx.draw_networkx(G, pos, with_labels=True, font_weight='bold', font_color='white', font_size=20, node_size=800)\n",
    "labels = nx.get_edge_attributes(G,'reward')\n",
    "nx.draw_networkx_edge_labels(G,pos,edge_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/realdiganta/solving_openai/blob/master/FrozenLake8x8/frozenLake8x8.py\n",
    "def value_iteration(env, max_iterations=100000, lmbda=0.9, verbose=1):\n",
    "    stateValue = [0 for i in range(n_states)]\n",
    "    newStateValue = stateValue.copy()\n",
    "    iterations = 0\n",
    "    start_time = time.time()\n",
    "    all_state_values = []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        \n",
    "        if verbose>0: print(\"Iteration: {}\".format(i))\n",
    "        \n",
    "        for state in range(n_states):\n",
    "            action_values = []      \n",
    "            \n",
    "            for action in range(n_actions):\n",
    "                state_value = 0\n",
    "                \n",
    "                for s in range(len(env[state][action])):\n",
    "                    # print(s)\n",
    "                    prob, next_state, reward, done = env[state][action][s]\n",
    "                    \n",
    "                    if verbose>1: \n",
    "                        print(\"Iteration: {} State: {} Action: {} Prob: {} Next State: {} Reward: {}\".format(i, state, action, prob, next_state, reward))\n",
    "                    state_action_value = prob * (reward + lmbda*stateValue[next_state])\n",
    "                    state_value += state_action_value\n",
    "                \n",
    "                action_values.append(state_value)\n",
    "                best_action = np.argmax(np.asarray(action_values))\n",
    "                newStateValue[state] = action_values[best_action]\n",
    "        \n",
    "        # if i > 1000: \n",
    "        if verbose>0: print(\"Delta V: {}\".format(np.max(np.abs(np.array(stateValue) - np.array(newStateValue)))))\n",
    "        if np.max(np.abs(np.array(stateValue) - np.array(newStateValue))) < 0.0001:\n",
    "            break\n",
    "            \n",
    "        stateValue = newStateValue.copy()\n",
    "        all_state_values.append(sum(stateValue))\n",
    "        iterations += 1\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    return stateValue, iterations, duration , all_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/realdiganta/solving_openai/blob/master/FrozenLake8x8/frozenLake8x8.py\n",
    "def get_policy(env,stateValue, lmbda=0.9):\n",
    "    policy = [0 for i in range(n_states)]\n",
    "    for state in range(n_states):\n",
    "        action_values = []\n",
    "        for action in range(n_actions):\n",
    "            action_value = 0\n",
    "            # print(env.P[state][action])\n",
    "            for i in range(len(env[state][action])):\n",
    "                prob, next_state, r, _ = env[state][action][i]\n",
    "                # print(\"I: {} R: {}\".format(i, r))\n",
    "                action_value += prob * (r + lmbda * stateValue[next_state])\n",
    "            action_values.append(action_value)\n",
    "        best_action = np.argmax(np.asarray(action_values))\n",
    "        policy[state] = best_action\n",
    "        # print(\"State: {} Action Values: {} Best Action: {}\".format(state, action_values, best_action))\n",
    "    return policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 100\n",
    "n_actions = 4\n",
    "n_rewards = 1\n",
    "\n",
    "# select which state actions gets a reward\n",
    "# n_state_actions = n_actions * n_states\n",
    "# selected_state_actions = np.arange(n_state_actions)\n",
    "# random.shuffle(selected_state_actions)\n",
    "# selected_state_actions=selected_state_actions[:n_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateValues, iterations, duration, all_state_values = value_iteration(env1, max_iterations=100000, verbose=0)\n",
    "policy_value_iteration = get_policy(env1, stateValues)\n",
    "evaluation_data.append(('Randomized Small', 'Value Iteration', sum(stateValues), iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations)), 'V': all_state_values})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Custom Random MDP (Small) Value Iteration Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 10000\n",
    "n_actions = 10\n",
    "n_rewards = 2\n",
    "\n",
    "# select which state actions gets a reward\n",
    "# n_state_actions = n_actions * n_states\n",
    "# selected_state_actions = np.arange(n_state_actions)\n",
    "# random.shuffle(selected_state_actions)\n",
    "# selected_state_actions=selected_state_actions[:n_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateValues, iterations, duration, all_state_values = value_iteration(env2, max_iterations=100000, verbose=0)\n",
    "policy_value_iteration = get_policy(env2, stateValues)\n",
    "evaluation_data.append(('Randomized Large', 'Value Iteration', sum(stateValues), iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations)), 'V': all_state_values})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Custom Random MDP (Large) Value Iteration Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(stateValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def compute_vpi(pi, env, gamma):\n",
    "    # use pi[state] to access the action that's prescribed by this policy\n",
    "    a = np.identity(n_states) \n",
    "    b = np.zeros(n_states)\n",
    "    # print(a)\n",
    "    # print(b) \n",
    "    \n",
    "    for state in range(n_states):\n",
    "        for probability, nextstate, reward, done in env[state][pi[state]]:\n",
    "            a[state][nextstate] = a[state][nextstate] - gamma * probability\n",
    "            # print(probability * reward)\n",
    "            b[state] += probability * reward\n",
    "    \n",
    "    # print(a)\n",
    "    # print(b)\n",
    "    V = np.linalg.solve(a, b)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 100\n",
    "n_actions = 4\n",
    "n_rewards = 1\n",
    "\n",
    "# select which state actions gets a reward\n",
    "# n_state_actions = n_actions * n_states\n",
    "# selected_state_actions = np.arange(n_state_actions)\n",
    "# random.shuffle(selected_state_actions)\n",
    "# selected_state_actions=selected_state_actions[:n_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.arange(n_states) % n_actions\n",
    "Vpi = compute_vpi(pi, env1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def compute_qpi(vpi, env, gamma):\n",
    "    Qpi = np.zeros([n_states, n_actions])\n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            for probability, nextstate, reward, done in env[state][action]:\n",
    "                Qpi[state][action] += probability * (reward + gamma * vpi[nextstate]) \n",
    "    return Qpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qpi = compute_qpi(Vpi, env1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def policy_iteration(env, gamma, nIt):\n",
    "    Vs = []\n",
    "    pis = []\n",
    "    pi_prev = np.zeros(n_states, dtype='int')\n",
    "    pis.append(pi_prev)\n",
    "    start_time = time.time()\n",
    "    print(\"Iteration | # chg actions | V[0]\")\n",
    "    print(\"----------+---------------+---------\")\n",
    "    for it in range(nIt):        \n",
    "        \n",
    "        # you need to compute qpi which is the state-action values for current pi\n",
    "        vpi = compute_vpi(pis[-1], env, gamma=gamma)\n",
    "        qpi = compute_qpi(vpi, env, gamma=gamma)\n",
    "        pi = qpi.argmax(axis=1)\n",
    "        print(\"%4i      | %6i        | %6.5f\"%(it, (pi != pi_prev).sum(), vpi[0]))\n",
    "        Vs.append(vpi)\n",
    "        pis.append(pi)\n",
    "        pi_prev = pi\n",
    "\n",
    "        if it > 1 and np.max(np.abs(np.array(Vs[-1]) - np.array(Vs[-2]))) < 0.0001:\n",
    "            break\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    return Vs, pis, it, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_PI, pis_PI, iterations, duration = policy_iteration(env1, gamma=0.9, nIt=20)\n",
    "evaluation_data.append(('Randomized Small', 'Policy Iteration', sum(Vs_PI[-1]), iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations + 1)), 'V': np.sum(Vs_PI, axis=1)})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Custom random MDP (Small) Policy Iteration Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 10000\n",
    "n_actions = 10\n",
    "n_rewards = 2\n",
    "\n",
    "# select which state actions gets a reward\n",
    "# n_state_actions = n_actions * n_states\n",
    "# selected_state_actions = np.arange(n_state_actions)\n",
    "# random.shuffle(selected_state_actions)\n",
    "# selected_state_actions=selected_state_actions[:n_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.arange(n_states) % n_actions\n",
    "Vpi = compute_vpi(pi, env2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def compute_qpi(vpi, env, gamma):\n",
    "    Qpi = np.zeros([n_states, n_actions])\n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            for probability, nextstate, reward, done in env[state][action]:\n",
    "                Qpi[state][action] += probability * (reward + gamma * vpi[nextstate]) \n",
    "    return Qpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qpi = compute_qpi(Vpi, env2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/hollygrimm/markov-decision-processes/blob/master/lab1/Lab%201%20-%20Problem%202.ipynb\n",
    "def policy_iteration(env, gamma, nIt):\n",
    "    Vs = []\n",
    "    pis = []\n",
    "    pi_prev = np.zeros(n_states, dtype='int')\n",
    "    pis.append(pi_prev)\n",
    "    start_time = time.time()\n",
    "    print(\"Iteration | # chg actions | V[0]\")\n",
    "    print(\"----------+---------------+---------\")\n",
    "    for it in range(nIt):        \n",
    "        \n",
    "        # you need to compute qpi which is the state-action values for current pi\n",
    "        vpi = compute_vpi(pis[-1], env, gamma=gamma)\n",
    "        qpi = compute_qpi(vpi, env, gamma=gamma)\n",
    "        pi = qpi.argmax(axis=1)\n",
    "        print(\"%4i      | %6i        | %6.5f\"%(it, (pi != pi_prev).sum(), vpi[0]))\n",
    "        Vs.append(vpi)\n",
    "        pis.append(pi)\n",
    "        pi_prev = pi\n",
    "\n",
    "        if it > 1 and np.max(np.abs(np.array(Vs[-1]) - np.array(Vs[-2]))) < 0.0001:\n",
    "            break\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    return Vs, pis, it, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_PI, pis_PI, iterations, duration = policy_iteration(env2, gamma=0.9, nIt=20)\n",
    "evaluation_data.append(('Randomized Large', 'Policy Iteration', sum(Vs_PI[-1]), iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations + 1)), 'V': np.sum(Vs_PI, axis=1)})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Custom random MDP Policy Iteration Convergence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 100\n",
    "n_actions = 4\n",
    "n_rewards = 1\n",
    "\n",
    "# select which state actions gets a reward\n",
    "# n_state_actions = n_actions * n_states\n",
    "# selected_state_actions = np.arange(n_state_actions)\n",
    "# random.shuffle(selected_state_actions)\n",
    "# selected_state_actions=selected_state_actions[:n_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = np.zeros((n_states, n_actions))\n",
    "qtable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_episodes = 30000        # Total episodes\n",
    "learning_rate = 0.9           # Learning rate\n",
    "max_steps = 99999999                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.1            # Minimum exploration probability \n",
    "decay_rate = 0.01            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "reward_achieved = 0\n",
    "episode = 0\n",
    "iterations = 0\n",
    "duration = 0\n",
    "diffs = []\n",
    "sumqtable = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "while True:\n",
    "\n",
    "    # Reset the environment\n",
    "    state = 0\n",
    "    old_state = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    old_qtable = qtable.copy()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "            # print(\"Exploiting knowledge!\")\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = random.choice(np.arange(n_actions))\n",
    "            # print(\"Randomly exploring!\")\n",
    "\n",
    "        # print(\"Action: {}\".format(action))\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        info, new_state, reward, done = env1[state][action][0]\n",
    "        # print(\"Episode: {} Action: {} New State: {} R: {} Epsilon: {}\".format(episode, action, new_state, reward, epsilon))\n",
    "\n",
    "        # if done and reward == 0:\n",
    "        #     reward = -0.5\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        old_state = state\n",
    "        state = new_state\n",
    "        \n",
    "        # print(total_rewards)\n",
    "        # If done (if we're dead) : finish episode\n",
    "        if total_rewards == 10:\n",
    "            # print(\"Done\") \n",
    "            reward_achieved += 1\n",
    "            # print(old_state, action)\n",
    "            # print(qtable[old_state, action])\n",
    "            # print(learning_rate * (reward + gamma * np.max(qtable[state, :]) - qtable[old_state, action]))\n",
    "            # # Reduce epsilon (because we need less and less exploration)\n",
    "            # print(reward_achieved)\n",
    "            # print(decay_rate)\n",
    "            # print(np.exp(-decay_rate*reward_achieved))\n",
    "            epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*reward_achieved)\n",
    "            # print(\"Epsilon: {}\".format(epsilon)) \n",
    "            print(\"Episode: {} Action: {} New State: {} R: {} Epsilon: {}\".format(episode, action, new_state, reward, epsilon))\n",
    "            break\n",
    "        \n",
    "    rewards.append(total_rewards)\n",
    "    episode += 1\n",
    "    iterations += 1\n",
    "\n",
    "    sumqtable.append(np.sum(qtable))\n",
    "\n",
    "    # print(diffs[-9:])\n",
    "    diffs.append(np.max(np.abs(np.array(old_qtable) - np.array(qtable))))\n",
    "    if sum(rewards)>50 and sum(diffs[-9:]) < 0.01:\n",
    "        break\n",
    "\n",
    "    # if abs(epsilon - min_epsilon) < 0.0001:\n",
    "    #     break\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print (\"Number of rewards: \" +  str(sum(rewards)))\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "evaluation_data.append(('Randomized Small', 'Q-learning', np.sum(qtable) / n_actions, iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations)), 'V': sumqtable})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Custom Random MDP (Small) Q Learning Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = [np.argmax(i) for i in qtable]\n",
    "policy = np.argmax(qtable, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 10000\n",
    "n_actions = 10\n",
    "n_rewards = 2\n",
    "\n",
    "# select which state actions gets a reward\n",
    "# n_state_actions = n_actions * n_states\n",
    "# selected_state_actions = np.arange(n_state_actions)\n",
    "# random.shuffle(selected_state_actions)\n",
    "# selected_state_actions=selected_state_actions[:n_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = np.zeros((n_states, n_actions))\n",
    "qtable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_episodes = 30000        # Total episodes\n",
    "learning_rate = 0.9           # Learning rate\n",
    "max_steps = 99999999                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.1            # Minimum exploration probability \n",
    "decay_rate = 0.01            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# the following function was inspired by and adapted from: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "reward_achieved = 0\n",
    "episode = 0\n",
    "iterations = 0\n",
    "duration = 0\n",
    "difs = []\n",
    "sumqtable = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "while True:\n",
    "    # Reset the environment\n",
    "    state = 0\n",
    "    old_state = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    old_qtable = qtable.copy()\n",
    "\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "            # print(\"Exploiting knowledge!\")\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = random.choice(np.arange(n_actions))\n",
    "            # print(\"Randomly exploring!\")\n",
    "\n",
    "        # print(\"Action: {}\".format(action))\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        info, new_state, reward, done = env2[state][action][0]\n",
    "        # print(\"Episode: {} Action: {} New State: {} R: {} Epsilon: {}\".format(episode, action, new_state, reward, epsilon))\n",
    "\n",
    "        # if done and reward == 0:\n",
    "        #     reward = -0.5\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        old_state = state\n",
    "        state = new_state\n",
    "        \n",
    "        # print(total_rewards)\n",
    "        # If done (if we're dead) : finish episode\n",
    "        if total_rewards == 10:\n",
    "            # print(\"Done\") \n",
    "            reward_achieved += 1\n",
    "            # print(old_state, action)\n",
    "            # print(qtable[old_state, action])\n",
    "            # print(learning_rate * (reward + gamma * np.max(qtable[state, :]) - qtable[old_state, action]))\n",
    "            # # Reduce epsilon (because we need less and less exploration)\n",
    "            # print(reward_achieved)\n",
    "            # print(decay_rate)\n",
    "            # print(np.exp(-decay_rate*reward_achieved))\n",
    "            epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*reward_achieved)\n",
    "            # print(\"Epsilon: {}\".format(epsilon)) \n",
    "            print(\"Episode: {} Action: {} New State: {} R: {} Epsilon: {}\".format(episode, action, new_state, reward, epsilon))\n",
    "            break\n",
    "        \n",
    "    rewards.append(total_rewards)\n",
    "    episode += 1\n",
    "    iterations += 1\n",
    "\n",
    "    sumqtable.append(np.sum(qtable))\n",
    "\n",
    "    diffs.append(np.max(np.abs(np.array(old_qtable) - np.array(qtable))))\n",
    "    if sum(rewards)>50 and sum(diffs[-9:]) < 0.01:\n",
    "        break\n",
    "\n",
    "    # if abs(epsilon - min_epsilon) < 0.0001:\n",
    "    #     break\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print (\"Number of rewards: \" +  str(sum(rewards)))\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "evaluation_data.append(('Randomized Large', 'Q-learning', np.sum(qtable) / n_actions, iterations, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = pd.DataFrame({'Iteration': list(np.arange(iterations)), 'V': sumqtable})\n",
    "ax = sns.lineplot(x=\"Iteration\", y=\"V\", data=plt_data).set_title('Custom Random MDP (Large) Q Learning Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "a = []\n",
    "v = []\n",
    "i = []\n",
    "r = []\n",
    "for f in evaluation_data:\n",
    "    # print(f)\n",
    "    p.append(f[0])\n",
    "    a.append(f[1])\n",
    "    v.append(round(f[2],2))\n",
    "    i.append(f[3])\n",
    "    r.append(round(f[4], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    autosize=True,\n",
    "    width=1000,\n",
    "    height=400)\n",
    "fig = go.Figure(\n",
    "    data=[go.Table(header=dict(values=['Problem', 'Algorithm', 'Value', 'Iterations', 'Runtime']),\n",
    "    cells=dict(values=[p, a, v, i, r]))], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ml': conda)",
   "language": "python",
   "name": "python361064bitmlconda3291cc5c44564875b7e62800a0ae8d21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
